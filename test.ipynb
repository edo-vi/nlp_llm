{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Downloading necessary NLTK pakages, if not already present\n",
      "=== Loaded\n",
      "Using model ``llama-13b-chat``\n"
     ]
    }
   ],
   "source": [
    "from llamaapi import LlamaAPI\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "from Document import Document\n",
    "\n",
    "print(\"=== Downloading necessary NLTK pakages, if not already present\")\n",
    "\n",
    "d1 = nltk.download(\"punkt\", quiet=True)\n",
    "d2 = nltk.download(\"stopwords\", quiet=True)\n",
    "d3 = nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "\n",
    "_MODEL = \"llama-13b-chat\"\n",
    "_CONTEXT_SIZE = 128  # In MB\n",
    "\n",
    "# A bit of obfuscation to make crawlers' life miserable\n",
    "# It's just the a.pi k.ey, written so that it's not a clear string\n",
    "# also, `LlamaAPI' is decoupled from the actual string so as to make it even harder to parse it\n",
    "\n",
    "_a1 = \"AbZ\".split(\"b\")[1]\n",
    "_a2 = \"BSaNbNTlp0o0bILov9Z3U7XmnP4DhwrV24jgq\"\n",
    "_a3 = \"A7kX0SPThAArXd0jNZxQ2WZ\"\n",
    "_call = lambda x: LlamaAPI(x + _a1)\n",
    "\n",
    "llama = _call(f\"LL-{_a3}2l1{_a2}\")\n",
    "\n",
    "\n",
    "print(f\"=== Loaded\\nUsing model ``{_MODEL}``\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MB_size(path):\n",
    "    file_stats = os.stat(path)\n",
    "    return file_stats.st_size / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2302, 0.7643, 0.7959, 0.7959, 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = \"./test1.txt\"\n",
    "path2 = \"./test2.txt\"\n",
    "path3 = \"./test3.txt\"\n",
    "\n",
    "if get_MB_size(path1) > _CONTEXT_SIZE:\n",
    "    print(\"Over the limit\")\n",
    "document1 = Document(path=path1).make_bow()\n",
    "document2 = Document(path=path2).make_bow()\n",
    "document3 = Document(path=path3).make_bow()\n",
    "# document.counts()\n",
    "text1 = document1.text(escape=False)\n",
    "text2 = document2.text(escape=False)\n",
    "text3 = document3.text(escape=False)\n",
    "document1.cosine_distance(document2), document1.cosine_distance(\n",
    "    document3\n",
    "), document2.cosine_distance(document3), document3.cosine_distance(\n",
    "    document2\n",
    "), document3.cosine_distance(\n",
    "    document3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5761\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
      " Nullam ut ornare odio. Suspendisse vestibulum porta ligula, ut consectetur risus sagittis et. \n",
      " Ut magna tellus, placerat at pharetra vel, elementum a ligula. Fusce lacinia in quam et laoreet. \n",
      "\n",
      "Integer non massa vitae purus lobortis feugiat et in eros. Curabitur vel dapibus lacus. Ut ac vestibulum arcu. \n",
      " Suspendisse vitae ornare arcu. Maecenas ac leo faucibus felis gravida bibendum. Sed tincidunt ac velit ullamcorper elementum. \n",
      " Vivamus luctus scelerisque lobortis. Sed nec elementum ante. In enim lectus, rutrum quis turpis ut, sagittis vulputate tellus.\n",
      " Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.\n",
      " Etiam aliquet, tortor non scelerisque vehicula, massa elit volutpat tortor, sit amet cursus libero lorem sit amet quam.\n",
      " Aliquam fermentum elit mollis lobortis euismod. Phasellus justo erat, convallis quis tellus et, commodo porta diam.\n",
      " Sed ullamcorper eleifend dolor at ultrices. Ut sollicitudin luctus commodo. Praesent convallis mollis velit,\n",
      " vel faucibus ipsum venenatis non. Donec aliquam magna eu nunc sagittis consectetur. Etiam maximus id metus vitae venenatis.\n",
      " Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque efficitur viverra orci, sit amet volutpat leo aliquam et.\n",
      " Donec justo odio, porta et nulla eu, interdum mattis nisi. Integer et dolor et ante feugiat tempor. Sed consequat rhoncus nisl vitae fermentum. Nam euismod mi nisi, id lacinia justo tempor at. Sed eu felis at elit dignissim vestibulum vel vel nunc.\n",
      " Nunc tellus est, pretium non aliquet nec, convallis vitae ipsum. Pellentesque egestas urna non iaculis vehicula. Ut in nisi eu nisl egestas auctor. Duis sollicitudin quis felis et suscipit. Phasellus non nibh in est aliquet commodo. Suspendisse at nulla laoreet, hendrerit ante non, pellentesque dolor. Quisque purus tellus, pharetra eu nunc et, condimentum gravida eros. Fusce dapibus urna in arcu scelerisque, eget maximus augue iaculis. Maecenas interdum euismod tincidunt. Etiam semper ut dui nec ultrices. Cras nec felis facilisis, gravida justo nec, vehicula ante. Cras mollis massa a arcu pretium dignissim.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lorem = Document(path=\"./lorem.txt\")\n",
    "d1, d2, distance = lorem.find_split()\n",
    "print(distance)\n",
    "print(d1.text())\n",
    "print(d2.text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Request JSON Cell\n",
    "api_request_json = {\n",
    "    \"model\": _MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a serious assistant\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text2,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Make your request and handle the response\n",
    "# response = llama.run(api_request_json)\n",
    "# resp = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(resp[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
