

The ethics of artificial intelligence is the branch of the ethics of technology specific to Artificial intelligence
 Approaches 

Machine ethics


Machine ethics (or machine morality) is the field of research concerned with designing Moral agencyArtificial moral agents
Isaac Asimov considered the issue in the 1950s in his I Robot At the insistence of his editor John W Campbell Jr he proposed the Three Laws of Robotics to govern artificially intelligent systems  Much of his work was then spent testing the boundaries of his three laws to see where they would break down or where they would create paradoxical or unanticipated behavior His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances More recently academics and many governments have challenged the idea that AI can itself be held accountable A panel convened by the United Kingdom in 2010 revised Asimovs laws to clarify that AI is the responsibility either of its manufacturers or of its owner/operator

In 2009 during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne Switzerland robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource

Some experts and academics have questioned the use of robots for military combat especially when such robots are given some degree of autonomous functions They point to programs like the Language Acquisition Device which can emulate human interaction

Vernor Vinge has suggested that a moment may come when some computers are smarter than humans He calls this "Technological singularity
There are discussions on creating tests to see if an AI is capable of making ethical decisions Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low A proposed alternative test is one called the Ethical Turing Test which would improve on the current test by having multiple judges decide if the AIs decision is ethical or unethical

In 2009 academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become selfsufficient and able to make their own decisions They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy and to what degree they could use such abilities to possibly pose any threat or hazard They noted that some machines have acquired various forms of semiautonomy including being able to find power sources on their own and being able to independently choose targets to attack with weapons They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence" They noted that selfawareness as depicted in sciencefiction is probably unlikely but that there were other potential hazards and pitfalls

However there is one technology in particular that has the potential to make the idea of morally capable robots a reality In a paper on the acquisition of moral values by robots Nayef AlRodhan mentions the case of Neuromorphic engineering
In Moral Machines: Teaching Robots Right from Wrong Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern Normative ethics
According to a 2019 report from the Center for the Governance of AI at the University of Oxford 82% of Americans believe that robots and AI should be carefully managed Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks infringements on data privacy hiring bias autonomous vehicles and drones that do not require a human controller Similarly according to a fivecountry https://assetskpmgcom/content/dam/kpmg/au/pdf/2021/trustinaimultiplecountriespdf study by KPMG and the University of Queensland Australia in 2021 6679% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully

 Robot ethics 


The term "robot ethics" (sometimes "roboethics") refers to the morality of how humans design construct use and treat robots Robot ethics intersect with the ethics of AI Robots are physical machines whereas AI can be only software Not all robots function through AI systems and not all AI systems are robots Robot ethics considers how machines may be used to harm or benefit humans their impact on individual autonomy and their effects on social justice

 Ethics principles of artificial intelligence 
In the review of 84 ethics guidelines for AI 11 clusters of principles were found: transparency justice and fairness nonmaleficence responsibility privacy beneficence freedom and autonomy trust sustainability dignity solidarity

Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (Beneficence (ethics)
Transparency accountability and open source
Bill Hibbard argues that because AI will have such a profound effect on humanity AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts Ben Goertzel and David Hart created OpenCog as an Opensource software
Unfortunately making code open source does not make it comprehensible which by many definitions means that the AI code is not transparent The IEEE Standards Association has published a Technical standards
Not only companies but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency and through it human accountability This strategy has proven controversial as some worry that it will slow the rate of innovation Others argue that regulation leads to systemic stability more able to support innovation in the long term The OECD UN EU and many countries are presently working on strategies for regulating AI and finding appropriate legal frameworks

On June 26 2019 the European Commission HighLevel Expert Group on Artificial Intelligence (AI HLEG) published its "Policy and investment recommendations for trustworthy Artificial Intelligence" This is the AI HLEGs second deliverable after the April 2019 publication of the "Ethics Guidelines for Trustworthy AI" The June AI HLEG recommendations cover four principal subjects: humans and society at large research and academia the private sector and the public sector The European Commission claims that "HLEGs recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth prosperity and innovation as well as the potential risks involved" and states that the EU aims to lead on the framing of policies governing AI internationally To prevent harm in addition to regulation AIdeploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI and take accountability to mitigate the risks On 21 April 2021 the European Commission proposed the Artificial Intelligence Act

 Ethical challenges 

 Biases in AI systems 

File:Kamala Harris speaks about racial bias in artificial intelligence  20200423oggAI has become increasingly inherent in facial and speech recognition
Bias can creep into algorithms in many ways The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system For instance Amazons AIpowered recruitment tool was trained with its own recruitment data accumulated over the years during which time the candidates that successfully got the job were mostly white males Consequently the algorithms learned the (biased) pattern from the historical data and generated predictions for the present/future that these types of candidates are most likely to succeed in getting the job Therefore the recruitment decisions made by the AI system turn out to be biased against female and minority candidates Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias technical bias and emergent bias In natural language processing problems can arise from the text corpus — the source material the algorithm uses to learn about the relationships between different words

Large companies such as IBM Google etc have made efforts to research and address these biases One solution for addressing bias is to create documentation for the data used to train AI systems Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors monitoring processes identifying potential root causes for improper execution and other functions

The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law and as more people without a deep technical understanding are tasked with deploying it Some experts warn that algorithmic bias is already pervasive in many industries and that almost no one is making an effort to identify or correct it There are some opensourced tools by civil societies that are looking to bring more awareness to biased AI

Robot rights

"Robot rights" is the concept that people should have moral obligations towards their machines akin to human rights or animal rights It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity analogous to linking human rights with human duties before society These could include the Personhood	 
The American Heritage Dictionary of the English Language Fourth Edition
	 
</ref> The issue has been considered by the Institute for the Future and by the Department of Trade and Industry (United Kingdom)	 
Experts disagree on how soon specific and detailed laws on the subject will be necessary while Ray Kurzweil sets the date at 2029<ref>
	 


</ref> Another group of scientists meeting in 2007 supposed that at least 50 years had to pass before any sufficiently advanced system would exist<ref>https://webarchiveorg/web/20080522163926/http://wwwindependentcouk/news/science/thebigquestionshouldthehumanracebeworriedbytheriseofrobots446107html The Big Question: Should the human race be worried by the rise of robots? Independent Newspaper

</ref>
	 
The rules for the 2003 Loebner Prize competition envisioned the possibility of robots having rights of their own:

<blockquote>61 If in any given year a publicly available opensource Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry If no such body can be identified or if there is disagreement among two or more claimants the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess either in the United States of America or in the venue of the contest the Cash Award and Gold Medal in its own right </blockquote>
	 	
In October 2017 the android Sophia (robot)	 
The philosophy of Sentientism grants degrees of moral consideration to all sentient beings primarily humans and most nonhuman animals If artificial or alien intelligence show evidence of being Sentience	 
Joanna Bryson has argued that creating AI that requires rights is both avoidable and would in itself be unethical both as a burden to the AI agents and to human society

Artificial suffering
In 2020 professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering This was despite credible theories having outlined possible ways by which AI systems may became conscious such as Integrated information theory Edelman notes one exception had been Thomas Metzinger who in 2018 called for a global moratorium on further work that risked creating conscious AIs The moratorium was to run to 2050 and could be either extended or repealed early depending on progress in better understanding the risks and how to mitigate them Metzinger repeated this argument in 2021 highlighting the risk of creating an "explosion of artificial suffering" both as an AI might suffer in intense ways that humans could not understand and as replication processes may see the creation of huge quantities of artificial conscious instances Several labs have openly stated they are trying to create conscious AIs There have been reports from those with close access to AIs not openly intended to be self aware that consciousness may already have unintentionally emerged These include OpenAI founder Ilya Sutskever in February 2022 when he wrote that todays large neural nets may be "slightly conscious" In November 2022 David Chalmers argued that it was unlikely current large language models like GPT3 had experienced consciousness but also that he considered there to be a serious possibility that large language models may become conscious in the future

Threat to human dignity

Joseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care such as:
 A customer service representative (AI technology is already used today for telephonebased interactive voice response systems)
 A nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)
 A soldier
 A judge
 A police officer
 A therapist (as was proposed by Kenneth Colby in the 70s)

Weizenbaum explains that we require authentic feelings of empathy from people in these positions If machines replace them we will find ourselves alienated devalued and frustrated for the artificially intelligent system would not be able to simulate empathy Artificial intelligence if used in this way represents a threat to human dignity Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an "atrophy of the human spirit that comes from thinking of ourselves as computers"

Pamela McCorduck counters that speaking for women and minorities "Id rather take my chances with an impartial computer" pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all

Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism) To Weizenbaum these points suggest that AI research devalues human life<ref name"Weizenbaums critique">
 cite book   pp 132–144 
</ref>

AI founder John McCarthy (computer scientist)
 Liability for selfdriving cars 


As the widespread use of Selfdriving car
In another incident on March 18 2018 Elaine Herzberg was struck and killed by a selfdriving Uber in Arizona In this case the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway but it could not anticipate a pedestrian in the middle of the road This raised the question of whether the driver pedestrian the car company or the government should be held responsible for her death

Currently selfdriving cars are considered semiautonomous requiring the driver to pay attention and be prepared to take control if necessary Thus it falls on governments to regulate the driver who overrelies on autonomous features as well educate them that these are just technologies that while convenient are not a complete substitute Before autonomous cars become widely used these issues need to be tackled through new policies

Weaponization of artificial intelligence

Some experts and academics have questioned the use of robots for military combat especially when such robots are given some degree of autonomy On October 31 2019 the United States Department of Defenses Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the black box and understand the killchain process However a major concern is how the report will be implemented The US Navy has funded a report which indicates that as military robots become more complex there should be greater attention to implications of their ability to make autonomous decisions Some researchers state that autonomous robots might be more humane as they could make decisions more effectively

Within this last decade there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities "The results may be used when designing future military robots to control unwanted tendencies to assign responsibility to the robots" From a Consequentialism
There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a AI takeover
"If any major military power pushes ahead with the AI weapon development a global arms race is virtually inevitable and the endpoint of this technological trajectory is obvious: autonomous weapons will become the AK47
Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like "dumb robots going rogue or a network that develops a mind of its own" Huw Price a colleague of Rees at Cambridge has voiced a similar warning that humans might not survive when intelligence "escapes the constraints of biology" These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence

Regarding the potential for smarterthanhuman systems to be employed militarily the Open Philanthropy Project writes that these scenarios "seem potentially as important as the risks related to loss of control" but research investigating AIs longrun social impact have spent relatively little time on this concern: "this class of scenarios has not been a major focus for the organizations that have been most active in this space such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI) and there seems to have been less analysis and debate regarding them"

Opaque algorithms
Approaches like machine learning with neural networks can result in computers making decisions that they and the humans who programmed them cannot explain It is difficult for people to determine if such decisions are fair and trustworthy leading potentially to bias in AI systems going undetected or people rejecting the use of such systems This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence Explainable artificial intelligence encompasses both explainability and interpretability with explainability relating to summarizing neural network behavior and building user confidence while interpretability is defined as the comprehension of what a model has done or could do

Negligent or Deliberate Misuse of AI
A special case of the opaqueness of AI is that caused by it being anthropomorphised that is assumed to have humanlike characteristics resulting in misplaced conceptions of its moral agency This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system Some recent digital governance regulation such as the EUs AI Act is set out to rectify this by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability This includes potentially Information technology audit
Singularity


Many researchers have argued that by way of an "intelligence explosion" a selfimproving AI could become so powerful that humans would not be able to stop it from achieving its goals In his paper "Ethical Issues in Advanced Artificial Intelligence" and subsequent book Superintelligence: Paths Dangers Strategies philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction He claims that general superintelligence would be capable of independent initiative and of making its own plans and may therefore be more appropriately thought of as an autonomous agent Since artificial intellects need not share our human motivational tendencies it would be up to the designers of the superintelligence to specify its original motivations Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals many uncontrolled unintended consequences could arise It could kill off all other agents persuade them to change their behavior or block their attempts at interference

However instead of overwhelming the human race and leading to our destruction Bostrom has also asserted that superintelligence can help us solve many difficult problems such as disease poverty and environmental destruction and could help us to "enhance" ourselves

The sheer complexity of human value systems makes it very difficult to make AIs motivations humanfriendly AI researchers such as Stuart J Russell Bill Hibbard Shannon Vallor Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines

Institutions in AI policy & ethics
There are many organisations concerned with AI ethics and policy public and governmental as well as corporate and societal

Amazoncom Inc
The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input and accepts as members many professionals from within and without its organization

Traditionally government has been used by societies to ensure ethics are observed through legislation and policing There are now many efforts by national governments as well as transnational government and NGO
AI ethics work is structured by personal values and professional commitments and involves constructing contextual meaning through data and algorithms Therefore AI ethics work needs to be incentivized

 Intergovernmental initiatives 
 The European Commission has a HighLevel Expert Group on Artificial Intelligence On 8 April 2019 this published its "Ethics Guidelines for Trustworthy Artificial Intelligence" The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020 The European Commission also proposed the Artificial Intelligence Act
 The OECD established an OECD AI Policy Observatory
 In 2021 UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence the first global standard on the ethics of AI

 Governmental initiatives 
 In the United States the Obama administration put together a Roadmap for AI Policy The Obama Administration released two prominent white papers on the future and impact of AI In 2019 the White House through an executive memo known as the "American AI Initiative" instructed NIST the (National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019)
In January 2020 in the United States the Trump administrationThe Computing Community Consortium The Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI
 The NonHuman Party is running for election in New South Wales with policies around granting rights to robots animals and generally nonhuman entities whose intelligence has been overlooked
 In Russia the firstever Russian "Codex of ethics of artificial intelligence" for business was signed in 2021 It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank Yandex Rosatom Higher School of Economics Moscow Institute of Physics and Technology ITMO University Nanosemantics Rostelecom CIAN and others

 Academic initiatives 
There are three research institutes at the University of Oxford that are centrally focused on AI ethics The Future of Humanity Institute that focuses both on AI Safety and the Governance of AI The Institute for Ethics in AI directed by John Tasioulas whose primary goal among others is to promote AI ethics as a field proper in comparison to related applied ethics fields The Oxford Internet Institute directed by Luciano Floridi focuses on the ethics of nearterm AI technologies and ICTs
The Centre for Digital Governance at the Hertie School in Berlin was cofounded by Joanna Bryson to research questions of ethics and technology
The AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence Its interdisciplinary research focuses on the themes bias and inclusion labour and automation rights and liberties and safety and civil infrastructure
The Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment and policy
The Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility employment healthcare and sustainability
Barbara J Grosz the Higgins Professor of Natural Sciences at the Harvard John A Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard University
 NGO initiatives 
An international nonprofit organization Future of Life Institute held a 5 day conference in Asilomar Conference on Beneficial AI
 Private organizations 

 Algorithmic Justice League
 Black in AI
 Data for Black Lives
 Queer in AI

 Role and impact of fiction 

The role of fiction with regards to AI ethics has been a complex one One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically fiction has been prefiguring common tropes that have not only influenced goals and visions for AI but also outlined ethical questions and common fears associated with it During the second half of the twentieth and the first decades of the twentyfirst century popular culture in particular movies TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics Recently these themes have also been increasingly treated in literature beyond the realm of science fiction And as Carme Torras research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes in higher education science fiction is also increasingly used for teaching technologyrelated ethical issues in technological degrees

 History 

Historically speaking the investigation of moral and ethical implications of "thinking machines" goes back at least to the Age of Enlightenment
The Romanticism
 Impact on technological development 

While the anticipation of a future dominated by potentially indomitable technology has fueled the imagination of writers and film makers for a long time one question has been less frequently analyzed namely to what extent fiction has played a role in providing inspiration for technological development It has been documented for instance that the young Alan Turing saw and appreciated aforementioned Shaws play Back to Methuselah in 1933 (just 3 years before the publication of his first seminal paper which laid the groundwork for the digital computer) and he would likely have been at least aware of plays like RUR which was an international success and translated into many languages

One might also ask the question which role science fiction played in establishing the tenets and ethical implications of AI development: Isaac Asimov conceptualized his Three Laws of Robotics in the 1942 short story  "Runaround (story)
Science fiction has been grappling with ethical implications of AI developments for decades and thus provided a blueprint for ethical issues that might emerge once something akin to general artificial intelligence has been achieved: Spike Jonzes 2013 film Her (film)
The theme of coexistence with artificial sentient beings is also the theme of two recent novels: Machines Like Me by Ian McEwan published in 2019 involves among many other things a lovetriangle involving an artificial person as well as a human couple Klara and the Sun by Nobel Prize in Literature
 TV series 

While ethical questions linked to AI have been featured in science fiction literature and List of artificial intelligence films
 Future visions in fiction and games 

The movie The Thirteenth Floor suggests a future where simulated reality
The ethics of artificial intelligence is one of several core themes in BioWares Mass Effect series of games It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them Beyond the initial conflict the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story

Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings This is one of the few games that puts players in the bionic perspective which allows them to better consider the rights and interests of robots once a true artificial intelligence is created

Over time debates have tended to focus less and less on possibility and more on desirability as emphasized in the Hugo de GarisThe Artilect War
Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White in ways that distort perceptions of its risks and benefits

Researchers
columnslistTimnit Gebru
Joy Buolamwini
Deb Raji
Ruha Benjamin
Safiya Noble
Margaret Mitchell (scientist)Meredith Whittaker
Alison Adam
Seth Baum
Nick Bostrom
Joanna Bryson
Kate Crawford
Kate Darling
Luciano Floridi
Anja Kaspersen
Michael Kearns (computer scientist)Ray Kurzweil
Catherine Malabou
Ajung Moon
Vincent C Müller
Peter Norvig
Steve Omohundro
Stuart J Russell
Anders Sandberg
Mariarosaria Taddeo
John Tasioulas
Roman Yampolskiy
Eliezer Yudkowsky
Emily M Bender


Organisations
columnslistCenter for HumanCompatible Artificial Intelligence
Center for Security and Emerging Technology
Centre for the Study of Existential Risk
Future of Humanity Institute
Future of Life Institute
Machine Intelligence Research Institute
Responsible AI Institute
Partnership on AI
Leverhulme Centre for the Future of Intelligence
Institute for Ethics and Emerging Technologies
Oxford Internet Institute
https://aiethicspt/ AiEthics Portugal


See also

columnslistAI takeover
Artificial consciousness
Artificial general intelligence (AGI)
Computer ethics
Dead internet theory
Effective altruismLongterm future and global catastrophic risksEthics of uncertain sentience
Existential risk from artificial general intelligence
Human Compatible
Personhood
Philosophy of artificial intelligence
Regulation of artificial intelligence 
Robotic governanceSuperintelligence: Paths Dangers Strategies
Suffering risks


Notes


External links
 http://wwwieputmedu/ethicai/ Ethics of Artificial Intelligence at the Internet Encyclopedia of Philosophy
 https://platostanfordedu/entries/ethicsai/ Ethics of Artificial Intelligence and Robotics at the Stanford Encyclopedia of Philosophy
 
 http://newsbbccouk/1/hi/sci/tech/1809769stm BBC News: Games to take on a life of their own
 http://wwwdasbootorg/thorissonhtm Whos Afraid of Robots?  an article on humanitys fear of artificial intelligence
 https://webarchiveorg/web/20080418122849/http://wwwsouthernctedu/organizations/rccs/resources/research/introduction/bynum_shrt_histhtml A short history of computer ethics
 https://algorithmwatchorg/en/project/aiethicsguidelinesglobalinventory/ AI Ethics Guidelines Global Inventory by https://algorithmwatchorg Algorithmwatch
 





