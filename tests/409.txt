


File:02SandvigSeeingtheSort2014WEBpng
Algorithmic bias describes systematic and repeatable errors in a Computer System
For mapping this to ideas in statistical learning it is extremely important to note that the "bias" here is typically the variance in the biasvariance trade off in machine learning So it is actually the opposite of real machine learning bias It is variance the model is overfitting the data or data generation process In situations where there was no training data and or no consistent learning then the "algorithmic bias" is simply a measure of deviation according to some score

Bias can emerge from many factors including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded collected selected or used to train the algorithm For example algorithmic bias has been observed in Search engine bias
As algorithms expand their ability to organize society politics institutions and behavior sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world Because algorithms are often considered to be neutral and unbiased they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias) and in some cases reliance on algorithms can displace human responsibility for their outcomes Bias can enter into algorithmic systems as a result of preexisting cultural social or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the softwares initial design

Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech It has also arisen in criminal justice healthcare and hiring compounding existing racial socioeconomic and gender biases The relative inability of facial recognition technology to accurately identify darkerskinned faces has been linked to multiple wrongful arrests of black men an issue stemming from imbalanced datasets Problems in understanding researching and discovering algorithmic bias persist due to the proprietary nature of algorithms which are typically treated as trade secrets Even when full transparency is provided the complexity of certain algorithms poses a barrier to understanding their functioning Furthermore algorithms may change or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis In many cases even within a single website or application there is no single "algorithm" to examine but a network of many interrelated programs and data inputs even between users of the same service

 Definitions 
File:A computer program for evaluating forestry opportunities under three investment criteria (1969) (20385500690)jpgAlgorithms are Algorithm characterizations
Contemporary Social science
 Methods 
Bias can be introduced to an algorithm in several ways During the assemblage of a dataset data may be collected digitized adapted and entered into a database according to humandesigned cataloging criteria Next programmers assign priorities or Hierarchy
Beyond assembling and processing data bias can emerge as a result of design For example algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores) Meanwhile recommendation engines that work by associating users with similar users or that make use of inferred marketing traits might rely on inaccurate associations that reflect broad ethnic gender socioeconomic or racial stereotypes Another example comes from determining criteria for what is included and excluded from results This criteria could present unanticipated outcomes for search results such as with flightrecommendation software that omits flights that do not follow the sponsoring airlines flight paths

 History 

 Early critiques 
File:Used Punchcard (5151286161)jpgThe earliest computer programs were designed to mimic human reasoning and deductions and were deemed to be functioning when they successfully and consistently reproduced that human logic In his 1976 book Computer Power and Human Reason artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program but also from the way a program is coded

Weizenbaum wrote that Computer program
Finally he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results Weizenbaum warned against trusting decisions made by computer programs that a user doesnt understand comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss Crucially the tourist has no basis of understanding how or why he arrived at his destination and a successful arrival does not mean the process is accurate or reliable

An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St Georges University of London
In recent years when more algorithms started to use machine learning methods on real world data algorithmic bias can be found more often due to the bias existing in the data

 Contemporary critiques and responses 
Though welldesigned algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings cases of bias still occur and are difficult to predict and analyze The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design Decisions made by one designer or team of designers may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the programs output may be forgotten In theory these biases may create new patterns of behavior or "scripts" in relationship to specific technologies as the code Cybernetics
The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist a process described by author Clay Shirky as "algorithmic authority" Shirky uses the term to describe "the decision to regard as authoritative an unmanaged process of extracting value from diverse untrustworthy sources" such as search results This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public For example a list of news items selected and presented as "trending" or "popular" may be created based on significantly wider criteria than just their popularity

Because of their convenience and authority algorithms are theorized as a means of delegating responsibility away from humans This can have the effect of reducing alternative options compromises or flexibility

Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft which have cocreated a working group named Fairness Accountability
and Transparency in Machine Learning Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences In recent years the study of the Fairness Accountability
and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT<ref>Cite web
 Types 

 Preexisting 

Preexisting bias in an algorithm is a consequence of underlying social and institutional Ideology
An example of this form of bias is the British Nationality Act Program designed to automate the evaluation of new British citizens after the 1981 British Nationality Act In its attempt to transfer a particular logic into an algorithmic process the BNAP inscribed the logic of the British Nationality Act into its algorithm which would perpetuate it even if the act was eventually repealed

Another source of bias which has been called “label choice bias" arises when proxy measures are used to train algorithms that build in bias against certain groups For example a widelyused algorithm predicted health care costs as a proxy for health care needs and used predictions to allocate resources to help patients with complex health needs This introduced bias because Black patients have lower costs even when they are just as unhealthy as White patients Solutions to the "label choice bias" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict) so for the prior example instead of predicting cost researchers would focus on the variable of healthcare needs which is rather more significant Adjusting the target led to almost double the number of Black patients being selected for the program

 Technical 
File:Three Surveillance camerasjpgTechnical bias emerges through limitations of a program computational power its design or other constraint on the system Such bias can also be a restraint of design for example a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three as in an airline price display Another case is software that relies on randomness for fair distributions of results If the random number generation mechanism is not truly random it can introduce bias for example by skewing selections toward items at the end or beginning of a list

A decontextualized algorithm uses unrelated information to sort results for example a flightpricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines The opposite may also apply in which results are evaluated in contexts different from which they are collected Data may be collected without crucial external context: for example when facial recognition system
Lastly technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way For example software weighs data points to determine whether a defendant should accept a plea bargain while ignoring the impact of emotion on a jury Another unintended result of this form of bias was found in the plagiarismdetection software Turnitin which compares studentwritten texts to information found online and returns a probability score that the students work is copied Because the software compares long strings of text it is more likely to identify nonnative speakers of English than native speakers as the latter group might be better able to change individual words break up strings of plagiarized text or obscure copied passages through synonyms Because it is easier for native speakers to evade detection as a result of the technical constraints of the software this creates a scenario where Turnitin identifies foreignspeakers of English for plagiarism while allowing more nativespeakers to evade detection

 Emergent 
Emergent properties
In 1990 an example of emergent bias was identified in the software used to place US medical students into residencies the National Residency Match Program (NRMP)

Additional emergent biases include:

 Correlations 
Unpredictable correlations can emerge when large data sets are compared to each other For example data collected about webbrowsing patterns may align with signals marking sensitive data (such as race or sexual orientation) By selecting according to certain behavior or browsing patterns the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data

 Unanticipated uses 
Emergent bias can occur when an algorithm is used by unanticipated audiences For example machines may require that users can read write or understand numbers or relate to an interface using metaphors that they do not understand These exclusions can become compounded as biased or exclusionary technology is more deeply integrated into society

Apart from exclusion unanticipated uses may emerge from the end user relying on the software rather than their own knowledge In one example an unanticipated user group led to algorithmic bias in the UK when the British National Act Program was created as a Proof of concept
 Feedback loops 
Emergent bias may also create a feedback loop or recursion if data collected for an algorithm results in realworld responses which are fed back into the algorithm For example simulations of the predictive policing software (PredPol) deployed in Oakland California suggested an increased police presence in black neighborhoods based on crime data reported by the public The simulation showed that the public reported crime based on the sight of police cars regardless of what police were doing The simulation interpreted police car sightings in modeling its predictions of crime and would in turn assign an even larger increase of police presence within those neighborhoods The Human Rights Data Analysis Group which conducted the simulation warned that in places where racial discrimination is a factor in arrests such feedback loops could reinforce and perpetuate racial discrimination in policing Another well known example of such an algorithm exhibiting such behavior is COMPAS (software)
Recommender systems such as those used to recommend online videos or news articles can create feedback loops When users click on content that is suggested by algorithms it influences the next set of suggestions Over time this may lead to users entering a filter bubble and being unaware of important or useful content

 Impact 

 Commercial influences 
Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies without the knowledge of a user who may mistake the algorithm as being impartial For example American Airlines created a flightfinding algorithm in the 1980s The software presented a range of flights from various airlines to customers but weighed factors that boosted its own flights regardless of price or convenience In testimony to the United States Congress the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment

In a 1998 paper describing Google the founders of the company had adopted a policy of transparency in search results regarding paid placement arguing that "advertisingfunded search engines will be inherently biased towards the advertisers and away from the needs of the consumers" This bias would be an "invisible" manipulation of the user

 Voting behavior 
A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20% The researchers concluded that candidates have "no means of competing" if an algorithm with or without intent boosted page listings for a rival candidate Facebook users who saw messages related to voting were more likely to vote A 2010 randomized trial of Facebook users showed a 20% increase (340000 votes) among users who saw messages encouraging voting as well as images of their friends who had voted Legal scholar Jonathan Zittrain has warned that this could create a "digital gerrymandering" effect in elections "the selective presentation of information by an intermediary to meet its agenda rather than to serve its users" if intentionally manipulated

 Gender discrimination 
In 2016 the professional networking site LinkedIn was discovered to recommend male variations of womens names in response to search queries The site did not make similar recommendations in searches for male names For example "Andrea" would bring up a prompt asking if users meant "Andrew" but queries for "Andrew" did not ask if users meant to find "Andrea" The company said this was the result of an analysis of users interactions with the site

In 2012 the department store franchise Target (company)
Web search algorithms have also been accused of bias Googles results may prioritize pornographic content in search terms related to sexuality for example "lesbian" This bias extends to the search engine showing popular but sexualized content in neutral searches For example "Top 25 Sexiest Women Athletes" articles displayed as firstpage results in searches for "women athletes" In 2017 Google adjusted these results along with others that surfaced hate groups racist views child abuse and pornography and other upsetting and offensive content Other examples include the display of higherpaying jobs to male applicants on job search websites Researchers have also identified that machine translation exhibits a strong tendency towards male defaults In particular this is observed in fields linked to unbalanced gender distribution including Science technology engineering and mathematics
In 2015 Amazoncom turned off an AI system it developed to screen job applications when they realized it was biased against women The recruitment tool excluded applicants who attended allwomens colleges and resumes that included the word "womens" A similar problem emerged with music streaming services—In 2019 it was discovered that the recommender system algorithm used by Spotify was biased against women artists Spotifys song recommendations suggested more male artists over women artists

 Racial and ethnic discrimination 
Algorithms have been criticized as a method for obscuring racial prejudices in decisionmaking Because of how certain races and ethnic groups were treated in the past data can often contain hidden biases For example black people are likely to receive longer sentences than white people who committed the same crime This could potentially mean that a system amplifies the original biases in the data

In 2015 Google apologized when black users complained that an imageidentification algorithm in its Photos application identified them as Ethnic stereotype
Biometric data about race may also be inferred rather than observed For example a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records regardless of whether there is any police record of that individuals name A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithms model of lung function

In 2019 a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients The algorithm predicts how much patients would cost the healthcare system in the future However cost is not raceneutral as black patients incurred about $1800 less in medical costs per year than white patients with the same number of chronic conditions which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases

A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on "creditworthiness" which is rooted in the US fairlending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans These particular algorithms were present in FinTech companies and were shown to discriminate against minorities

 Law enforcement and legal proceedings 
Algorithms already have numerous applications in legal systems An example of this is COMPAS (software)
One example is the use of risk assessments in criminal sentencing in the United States and Parole board
One study that set out to examine "Risk Race & Recidivism: Predictive Bias and Disparate Impact" alleges a twofold (45 percent vs 23 percent) adverse likelihood for black vs Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a twoyear period of observation 

In the pretrial detention context a law review article argues that algorithmic risk assessments violate Fourteenth Amendment to the United States Constitution
 Online hate speech 
In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content according to internal Facebook documents The algorithm which is a combination of computer programs and human content reviewers was created to protect broad categories rather than specific subsets of categories For example posts denouncing "Muslims" would be blocked while posts denouncing "Radical Muslims" would be allowed An unanticipated outcome of the algorithm is to allow hate speech against black children because they denounce the "children" subset of blacks rather than "all blacks" whereas "all white men" would trigger a block because whites and males are not considered subsets

While algorithms are used to track and block hate speech some were found to be 15 times more likely to flag information posted by Black users and 22 times likely to flag information as hate speech if written in African American English Without context for slurs and epithets even when used by communities which have reappropriated them were flagged

 Surveillance 
Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors and to determine who belongs in certain locations at certain times However even audits of these imagerecognition systems are ethically fraught and some scholars have suggested the technologys context will always have a disproportionate impact on communities whose actions are oversurveilled For example a 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases The software was assessed as identifying men more frequently than women older people more frequently than the young and identified Asians AfricanAmericans and other races more often than whites

 Discrimination against the LGBTQ community 
In 2011 users of the gay hookup application Grindr reported that the Google Play
In 2019 it was found that on Facebook searches for "photos of my female friends" yielded suggestions such as "in bikinis" or "at the beach" In contrast searches for "photos of my male friends" yielded no results

Facial recognition technology has been seen to cause problems for transgender individuals In 2018 there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a builtin security measure As a result of this some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos which created an issue of violation of privacy

There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individuals sexual orientation based on their facial images The model in the study predicted a correct distinction between gay and straight men 81% of the time and a correct distinction between gay and straight women 74% of the time This study resulted in a backlash from the LGBTQIA community who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being "outed" against their will

 Disability discrimination 
While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias – like gender race and socioeconomic status disability often is left out of the list The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms creating even more exclusion   

The shifting nature of disabilities and its subjective characterization makes it more difficult to computationally address The lack of historical depth in defining disabilities collecting its incidence and prevalence in questionnaires and establishing recognition add to the controversy and ambiguity in its quantification and calculations  The definition of disability has been long debated shifting from a Medical model of disability
Given the stereotypes and stigmas that still exist surrounding disabilities the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges  As disclosing disability information can be taboo and drive further discrimination against this population there is a lack of explicit disability data available for algorithmic systems to interact with People with disabilities face additional harms and risks with respect to their social support cost of health insurance workplace discrimination and other basic necessities upon disclosing their disability status Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures 

 Google Search 
While users generate results that are "completed" automatically Google has failed to remove sexist and racist autocompletion text For example Algorithms of Oppression
 Obstacles to research 

Several problems impede the study of largescale algorithmic bias hindering the application of academically rigorous studies and public understanding

 Defining fairness 


Literature on algorithmic bias has focused on the remedy of fairness but definitions of fairness are often incompatible with each other and the realities of machine learning optimization For example defining fairness as an "equality of outcomes" may simply refer to a system producing the same result for all people while fairness defined as "equality of treatment" might explicitly consider differences between individuals As a result fairness is sometimes described as being in conflict with the accuracy of a model suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems In response to this tension researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms with "fairness" defined for specific applications and contexts

 Complexity 
Algorithmic processes are Complex system
An example of this complexity can be found in the range of inputs into customizing feedback The social media site Facebook factored in at least 100000 data points to determine the layout of a users social media feed in 2013 Furthermore large teams of programmers may operate in relative isolation from one another and be unaware of the cumulative effects of small decisions within connected elaborate algorithms

Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks time spent on site and other metrics These personal adjustments can confuse general attempts to understand algorithms One unidentified streaming radio service reported that it used five unique musicselection algorithms it selected for its users based on their behavior This creates different experiences of the same streaming services between different users making it harder to understand what these algorithms do
Companies also run frequent A/B tests to finetune algorithms based on user response For example the search engine Bing (search engine)
 Lack of transparency 
Commercial algorithms are proprietary and may be treated as trade secrets

 Lack of data about sensitive categories 
A significant barrier to understanding the tackling of bias in practice is that categories such as demographics of individuals protected by antidiscrimination law are often not explicitly considered when collecting and processing data In some cases there is little opportunity to collect this data explicitly such as in device fingerprinting ubiquitous computing and the Internet of things
Some practitioners have tried to estimate and impute these missing sensitive categorisations in order to allow bias mitigation for example building systems to infer ethnicity from names however this can introduce other forms of bias if not undertaken with care Machine learning researchers have drawn upon cryptographic privacyenhancing technologies such as secure multiparty computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext

Algorithmic bias does not only include protected categories but can also concern characteristics less easily observable or codifiable such as political viewpoints In these cases there is rarely an easily accessible or noncontroversial ground truth and removing the bias from such a system is more difficult Furthermore false and accidental correlations can emerge from a lack of understanding of protected categories for example insurance rates based on historical data of car accidents which may overlap strictly by coincidence with residential clusters of ethnic minorities

Solutions
A study of 84 policy guidelines on ethical AI found that fairness and "mitigation of unwanted bias" was a common point of concern and were addressed through a blend of technical solutions transparency and monitoring right to remedy and increased oversight and diversity and inclusion efforts

 Technical 


There have been several attempts to create methods and tools that can detect and observe biases within an algorithm These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithms internal processes These methods may also analyze a programs output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion) Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model Using machine learning to detect bias is called "conducting an AI audit" where the "auditor" is an algorithm that goes through the AI model and the training data to identify biases
Ensuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information
from its input signals because this is typically implicit in other signals For example the hobbies sports and schools attended
by a job candidate might reveal their gender to the software even when this is removed from the analysis Solutions to this
problem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected
and sensitive information about the subject as first demonstrated in  where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature A simpler method was proposed in the context of word embeddings and involves removing information that is correlated with the protected characteristic

Currently a new IEEE Standards Association
 Transparency and monitoring 


Ethics guidelines on AI point to the need for accountability recommending that steps be taken to improve the interpretability of results Such solutions include the consideration of the "right to understanding" in machine learning algorithms and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed Toward this end a movement for "Explainable artificial intelligence
An initial approach towards transparency included the Opensource software
 Right to remedy 
From a regulatory perspective the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias This includes legislating expectations of due diligence on behalf of designers of these algorithms and creating accountability when private actors fail to protect the public interest noting that such rights may be obscured by the complexity of determining responsibility within a web of complex intertwining processes Others propose the need for clear liability insurance mechanisms

 Diversity and inclusion 
Amid concerns that the design of AI systems is primarily the domain of white male engineers a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems with black AI leaders pointing to a "diversity crisis" in the field Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality and have called for applying a more deliberate lens of Intersectional feminism
 Interdisciplinarity and Collaboration 
Integrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias Integrating insights expertise and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects PACT attempts to reify the importance of decolonizing and powershifting efforts in the design of humancentered AI solutions An academic initiative in this regard is the Stanford Universitys Institute for HumanCentered Artificial Intelligence which aims to foster multidisciplinary collaboration The mission of the institute is to advance artificial intelligence (AI) research education policy and practice to improve the human condition 

Collaboration with outside experts and various stakeholders facilitates ethical inclusive and accountable development of intelligent systems It incorporates ethical considerations understands the social and cultural context promotes humancentered design leverages technical expertise and addresses policy and legal considerations Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair transparent and accountable

 Regulation 

 Europe 
The General Data Protection Regulation (GDPR) the European Unions revised data protection regime that was implemented in 2018 addresses "Automated individual decisionmaking including Profiling (information science)
The GDPR addresses algorithmic bias in profiling systems as well as the statistical approaches possible to clean it directly in Recital (law)
 United States 
The United States has no general legislation controlling algorithmic bias approaching the problem through various state and federal laws that might vary by industry sector and by how an algorithm is used Many policies are selfenforced or controlled by the Federal Trade Commission which was intended to guide policymakers toward a critical assessment of algorithms It recommended researchers to "design these systems so that their actions and decisionmaking are transparent and easily interpretable by humans and thus can be examined for any bias they may contain rather than just learning and repeating these biases" Intended only as guidance the report did not create any legal precedent

In 2017 New York City passed the first algorithmic accountability bill in the United States The bill which went into effect on January 1 2018 required "the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public and how agencies may address instances where people are harmed by agency automated decision systems" The task force is required to present findings and recommendations for further regulatory action in 2019

 India 
On July 31 2018 a draft of the Personal Data Bill was presented The draft proposes standards for the storage processing and transmission of data While it does not use the term algorithm it makes for provisions for "harm resulting from any processing or any kind of processing undertaken by the fiduciary" It defines "any denial or withdrawal of a service benefit or good resulting from an evaluative decision about the data principal" or "any discriminatory treatment" as a source of harm that could arise from improper use of data It also makes special provisions for people of "Intersex status"

 See also 
 Ethics of artificial intelligence
 Fairness (machine learning)
 Hallucination (artificial intelligence)
 Misaligned goals in artificial intelligence
 Predictive policing
 SenseTime

 References 


 Further reading 
 
 

