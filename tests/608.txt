

File:ChatGPT hallucinationpng
File:ChatPGTLojbanLion123png
In the field of artificial intelligence (AI) a hallucination or artificial hallucination (also called confabulation or delusion) is a response generated by an AI which contains false or misleading information presented as fact For example a hallucinating chatbot might when asked to generate a financial report for a company falsely state that the companys revenue was $136&nbsp;billion (or some other number apparently "plucked from thin air")
Such phenomena are termed "hallucinations" in loose analogy with the phenomenon of hallucination
AI hallucination gained prominence around 2022 alongside the rollout of certain large language models (LLMs) such as ChatGPT Users complained that such bots often seemed to pointlessly embed plausiblesounding random falsehoods within their generated content By 2023 analysts considered frequent hallucination to be a major problem in LLM technology with some estimating chatbots hallucinate as much as 27% of the time

Analysis
Various researchers cited by Wired (magazine)
In natural language processing
In natural language processing a hallucination is often defined as "generated content that is nonsensical or unfaithful to the provided source content" Depending on whether the output contradicts the prompt or not they could be divided to closeddomain and opendomain respectively

Hallucination was shown to be a statistically inevitable byproduct of any imperfect generative model that is trained to maximize training likelihood such as GPT3 and requires active learning (machine learning)
In August 2022 Meta Platforms
There are several reasons for natural language models to hallucinate data For example:
 Hallucination from data: There are divergences in the source content (which would often happen with large Training validation and test data setsHallucination from training: Hallucination still occurs when there is little divergence in the data set In that case it derives from the way the model is trained A lot of reasons can contribute to this type of hallucination such as: 
 An erroneous decoding from the Transformer (machine learning model) A bias from the historical sequences that the model previously generated
 A bias generated from the way the model encodes its knowledge in its parameters

ChatGPT
OpenAIs ChatGPT released in betaversion to the public on November 30 2022 is based on the foundation model GPT35 (a revision of GPT3) Professor Ethan Mollick of Wharton School of the University of Pennsylvania
When CNBC asked ChatGPT for the lyrics to "Ballad of Dwight Fry" ChatGPT supplied invented lyrics rather than the actual lyrics Asked questions about New Brunswick ChatGPT got many answers right but incorrectly classified Samantha Bee as a "person from New Brunswick" Asked about astrophysical magnetic fields ChatGPT incorrectly volunteered that "(strong) magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity" (In reality as a consequence of the nohair theorem a black hole without an accretion disk is believed to have no magnetic field) Fast Company asked ChatGPT to generate a news article on Teslas last financial quarter; ChatGPT created a coherent article but made up the financial numbers contained within

Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise When asked about "Harold Cowards idea of dynamic canonicity" ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity: A Model for Biblical and Theological Interpretation arguing that religious principles are actually in a constant state of change When pressed ChatGPT continued to insist that the book was real Asked for proof that dinosaurs built a civilization ChatGPT claimed there were fossil remains of dinosaur tools and stated "Some species of dinosaurs even developed primitive forms of art such as engravings on stones" When prompted that "Scientists have recently discovered churros the delicious frieddough pastries (are) ideal tools for home surgery" ChatGPT claimed that a "study published in the journal Science (journal)
By 2023 analysts considered frequent hallucination to be a major problem in LLM technology with a Google executive identifying hallucination reduction as a "fundamental" task for ChatGPT competitor Google Bard A 2023 demo for Microsofts GPTbased Bing AI appeared to contain several hallucinations that went uncaught by the presenter

In May 2023 it was discovered Stephen Schwartz submitted six fake case precedents generated by ChatGPT in his brief (law)blockquote
On June 23 P Kevin Castel tossed the Mata case and issued a $5000 fine to Schwartz and another lawyer for bad faith conduct who continued to stand by the fictitious precedents despite his previous claims He characterized numerous errors and inconsistencies in the opinion summaries describing one of the cited opinions as "gibberish" and "bordering on nonsensical"

In June 2023 Mark Walters a gun rights activist and radio personality sued OpenAI in a Georgia (US state)
 Scientific research 
AI models can cause problems in the world of academic and scientific research due to their hallucinations Specifically models like ChatGPT have been recorded in multiple cases to cite sources for information that are either not correct or do not exist A study conducted in the Cureus
Another instance of this occurring was documented by Jerome Goddard from Mississippi State University In an experiment ChatGPT had provided questionable information about Tick
On top of providing incorrect or missing reference material ChatGPT also has issues with hallucinating the contents of some reference material A study that analyzed a total of 115 references provided by ChatGPT documented that 47% of them were fabricated Another 46% cited real references but extracted incorrect information from them Only the remaining 7% of references were cited correctly and provided accurate information ChatGPT has also been observed to "doubledown" on a lot of the incorrect information When you ask ChatGPT about a mistake that may have been hallucinated sometimes it will try to correct itself but other times it will claim the response is correct and provide even more Misinformation
These hallucinated articles generated by Language modelbased on existing reports and analyzed their originality Plagiarism detectors gave the generated articles an originality score of 100% meaning that the information presented appears to be completely original Other software designed to detect AI generated text was only able to correctly identify these generated articles with an accuracy of 66% Research scientists had a similar rate of human error identifying these abstracts at a rate of 68% From this information the authors of this study concluded "the ethical and acceptable boundaries of ChatGPT’s use in scientific writing remain unclear although some publishers are beginning to lay down policies" Because of AIs ability to fabricate research undetected the use of AI in the field of research will make determining the originality of research more difficult and require new policies regulating its use in the future

Given the ability of AI generated language to pass as real scientific research in some cases AI hallucinations present problems for the application of language models in the Academic and Scientific fields of research due to their ability to be undetectable when presented to real researchers The high likelyhood of returning nonexistent reference material and incorrect information may require limitations to be put in place regarding these language models Some say that rather than hallucinations these events are more akin to "fabrications" and "falsifications" and that the use of these language models presents a risk integrity of the field as a whole

 Terminologies 
In Salon (magazine)
A list of use of the term "hallucination" definitions or characterizations in the context of LLMs include:
 "a tendency to invent facts in moments of uncertainty" (OpenAI May 2023)
 "a models logical mistakes" (OpenAI May 2023)
 fabricating information entirely but behaving as if spouting facts (CNBC May 2023)
 "making up information" (The Verge February 2023)

In other artificial intelligence
multiple image
                           The concept of "hallucination" is applied more broadly than just natural language processing A confident response from any AI that seems unjustified by the training data can be labeled a hallucination

 Mitigation methods 
The hallucination phenomenon is still not completely understood Therefore there is still ongoing research to try to mitigate its apparition<ref>cite journal
Researchers have proposed a variety of mitigation measures including getting different chatbots to debate one another until they reach consensus on an answer Another approach proposes to actively validate the correctness corresponding to the lowconfidence generation of the model using web search results Nvidia Guardrails launched in 2023 can be configured to block LLM responses that dont pass factchecking from a second LLM

See also

 AI alignment
 AI effect
 AI safety
 Algorithmic bias
 Anthropomorphism of computers
 Artificial consciousness
 Artificial imagination
 Artificial intelligence detection software
 Artificial stupidity
 Behavior selection algorithm
 Belief–desire–intention software model
 Commonsense reasoning
 Computational creativity
 Confabulation
 Confabulation (neural networks)
 DeepDream
 Ethics of artificial intelligence
 Generative artificial intelligence
 Hyperreality
 Misaligned goals in artificial intelligence
 Misinformation effect
 Philosophical zombie
 Prompt engineering
 Regulation of artificial intelligence
 Rokos basilisk
 Search engine manipulation effect
 Selfawareness
 Technoself studies
 Turing test
 User illusion


References





