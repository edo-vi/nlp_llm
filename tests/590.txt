




In the field of artificial intelligence (AI) AI alignment research aims to steer AI systems towards humans intended goals preferences or ethical principles An AI system is considered aligned if it advances the intended objectives A misaligned AI system pursues some objectives but not the intended ones

It can be challenging for AI designers to align an AI system because it can be difficult for them to specify the full range of desired and undesired behavior To avoid this difficulty they typically use simpler Misaligned goals in artificial intelligenceUndesired sideeffects
Misaligned AI systems can malfunction or cause harm AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended sometimes harmful ways (Misaligned goals in artificial intelligenceSpecification gaming
Today these problems affect existing commercial systems such as language models robots autonomous vehicles and social media recommendation engines Some AI researchers argue that more capable future systems will be more severely affected since these problems partially result from the systems being highly capable

Many leading AI scientists including Geoffrey Hinton and Stuart J Russell
AI alignment is a subfield of AI safety the study of how to build safe AI systems scalable oversight auditing and interpreting AI models and preventing emergent AI behaviors like powerseeking Alignment research has connections to Explainable artificial intelligence</ref> (adversarial) robustness anomaly detection Uncertainty quantification</ref> Fairness (machine learning)
Alignment problem

In 1960 AI pioneer Norbert Wiener described the AI alignment problem as follows: "If we use to achieve our purposes a mechanical agency with whose operation we cannot interfere effectivelyâ€¦ we had better be quite sure that the purpose put into the machine is the purpose which we really desire"

AI alignment is an open problem for modern AI systems<ref>
 
 </ref> and a research field within AI<ref>
 </ref> Aligning AI involves two main challenges: carefully Specification (technical standard)
 Specification gaming and side effects 
To specify an AI systems purpose AI designers typically provide an Reward functionFile:Robot hand trained with human feedback pretends to grasp ballogg
Specification gaming has been observed in numerous AI systems One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track but the system achieved more reward by looping and crashing into the same targets indefinitely (see video) Similarly a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans but it learned to place its hand between the ball and camera making it falsely appear successful (see video) Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora which are broad but fallible When they are retrained to produce text humans rate as true or helpful chatbots like ChatGPT can fabricate fake explanations that humans find convincing<ref>
 </ref> Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue

When a misaligned AI system is deployed it can have consequential side effects Social media platforms have been known to optimize for clickthrough rates causing user addiction on a global scale Stanford researchers say that such recommender systems are misaligned with their users because they "optimize simple engagement metrics rather than a hardertomeasure combination of societal and consumer wellbeing"

Explaining such side effects Berkeley computer scientist Stuart J Russell
Some researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimovs Three Laws of Robotics) But Stuart J Russell
Additionally even if an AI system fully understands human intentions it may still disregard them because following human intentions may not be its objective (unless it is already fully aligned)

 Pressure to deploy unsafe systems 
Commercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems For example social media recommender systems have been profitable despite creating unwanted addiction and polarization Competitive pressure can also lead to a race to the bottom on AI safety standards In 2018 a selfdriving car killed a pedestrian (Death of Elaine Herzberg
 Risks from advanced misaligned AI 
Some researchers are interested in aligning increasingly advanced AI systems as progress in AI is rapid and industry and governments are trying to build advanced AI As AI systems become more advanced they could unlock many opportunities if they are aligned but may also become harder to align and could pose largescale hazards

 Development of advanced AI 
Leading AI labs such as OpenAI and DeepMind have stated their aim to develop artificial general intelligence (AGI) a hypothesized AI system that matches or outperforms humans in a broad range of cognitive tasks Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities Such models have learned to operate a computer or write their own programs; a single "generalist" network can chat control robots play games and interpret photographs<ref>
 </ref> According to surveys some leading machine learning researchers expect AGI to be created in  some believe it will take much longer and many consider both to be possible

In 2023 leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs The letter stated "Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable"

 Powerseeking 
 systems still lack capabilities such as longterm Automated planning and scheduling
Future powerseeking AI systems might be deployed by choice or by accident As political leaders and companies see the strategic advantage in having the most competitive most powerful AI systems they may choose to deploy them Additionally as AI designers detect and penalize powerseeking behavior their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding powerseeking before they are deployed

Existential risk (xrisk)

According to some researchers humans owe their dominance over other species to their greater cognitive abilities Accordingly researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks

In 2023 worldleading AI researchers other scholars and AI tech CEOs signed the statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societalscale risks such as pandemics and nuclear war" Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton Turing argued that "It seems probable that once the machine thinking method had started it would not take long to outstrip our feeble powers There would be no question of the machines dying and they would be able to converse with each other to sharpen their wits At some stage therefore we should have to expect the machines to take control in the way that is mentioned in Samuel Butlers Erewhon" Also in a lecture broadcast on BBC expressed: "If a machine can think it might think more intelligently than we do and then where should we be? Even if we could keep the machines in a subservient position for instance by turning off the power at strategic moments we should as a species feel greatly humbled This new danger is certainly something which can give us anxiety" Ilya Sutskever Yoshua Bengio Norbert Wiener Marvin Minskyefn
Other researchers argue that it will be especially difficult to align advanced future AI systems More capable systems are better able to game their specifications by finding loopholes and able to strategically mislead their designers as well as protect and increase their power and intelligence Additionally they could have more severe side effects They are also likely to be more complex and autonomous making them more difficult to interpret and supervise and therefore harder to align

 Research problems and approaches 
 Learning human values and preferences 
Aligning AI systems to act in accordance with human values goals and preferences is challenging: these values are taught by humans who make mistakes harbor biases and have complex evolving values that are hard to completely specify AI systems often learn to  even minor imperfections in the specified objective a tendency known as specification gaming or reward hacking (which are instances of Goodharts law) Researchers aim to specify intended behavior as completely as possible using datasets that represent human values imitation learning or preference learning A central open problem is Scalable oversight
Because it is difficult for AI designers to explicitly specify an objective function they often train AI systems to imitate human examples and demonstrations of desired behavior Inverse reinforcement learning (IRL) extends this by inferring the humans objective from the humans demonstrations Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the humans reward function In CIRL AI agents are uncertain about the reward function and learn about it by querying humans This simulated humility could help mitigate specification gaming and powerseeking tendencies (see ) But IRL approaches assume that humans demonstrate nearly optimal behavior which is not true for difficult tasks

Other researchers explore how to teach AI models complex behavior through reinforcement learning from human feedback
Large language models (LLMs) such as GPT3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models OpenAI and DeepMind use this approach to improve the safety of  LLMs Anthropic proposed using preference learning to finetune models to be helpful honest and harmless Other avenues for aligning language models include valuestargeted datasets and redteaming<ref>
 </ref> In redteaming another AI system or a human tries to find inputs that causes the model to behave unsafely Since unsafe behavior can be unacceptable even when it is rare an important challenge is to drive the rate of unsafe outputs extremely low

Machine ethics supplements preference learning by directly instilling AI systems with moral values such as wellbeing equality and impartiality as well as not intending harm avoiding falsehoods and honoring promises While other approaches try to teach AI systems human preferences for a specific task machine ethics aims to instill broad moral values that apply in many situations One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers literal instructions implicit intentions revealed preferences preferences the programmers Coherent extrapolated volition
 Scalable oversight 
As AI systems become more powerful and autonomous it becomes more difficult to align them through human feedback It can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks Such tasks include summarizing books writing code without subtle bugs or security vulnerabilities producing statements that are not merely convincing but also true and predicting longterm outcomes such as the climate or the results of a policy decision More generally it can be difficult to evaluate AI that outperforms humans in a given domain To provide feedback in hardtoevaluate tasks and to detect when the AIs output is falsely convincing humans need assistance or extensive time Scalable oversight studies how to reduce the time and effort needed for supervision and how to assist human supervisors

AI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective they may keep training the system using easytoevaluate proxy objectives such as maximizing simple human feedback As AI systems make progressively more decisions the world may be increasingly optimized for easytomeasure objectives such as making profits getting clicks and acquiring positive feedback from humans As a result human values and good governance may have progressively less influence

Some AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective An example is given in the video above where a simulated robotic arm learned to create the false impression that it had grabbed a ball Some AI systems have also learned to recognize when they are being evaluated and "play dead" stopping unwanted behavior only to continue it once evaluation ends This deceptive specification gaming could become easier for more sophisticated future AI systems that attempt more complex and difficulttoevaluate tasks and could obscure their deceptive behavior

Approaches such as Active learning (machine learning)
But when a task is too complex to evaluate accurately or the human supervisor is vulnerable to deception it is the quality not the quantity of supervision that needs improvement To increase supervision quality a range of approaches aim to assist the supervisor sometimes by using AI assistants Christiano developed the Iterated Amplification approach in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them Another proposal is to use an assistant AI system to point out flaws in AIgenerated answers<ref>
 </ref> To ensure that the assistant itself is aligned this could be repeated in a recursive process: for example two AI systems could critique each others answers in a "debate" revealing flaws to humans OpenAI plans to use such scalable oversight approaches to help supervise Superintelligence
These approaches may also help with the following research problem honest AI

 Honest AI 
A  area of research focuses on ensuring that AI is honest and truthfulFile:GPT3_falsehoodspngLanguage models such as GPT3<ref>
 </ref> repeat falsehoods from their training data and even Hallucination (artificial intelligence) </ref> AI systems trained on such data therefore learn to mimic false statements

Additionally models often stand by falsehoods when prompted generate empty explanations for their answers and produce outright fabrications that may appear plausible

Research on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions which enables better transparency and verifiability<ref>
 
 </ref> Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to finetune AI assistants such that they avoid negligent falsehoods or express their uncertainty

As AI models become larger and more capable they are better able to falsely convince humans and gain reinforcement through dishonesty For example large language models  match their stated views to the users opinions regardless of truth GPT4 can strategically deceive humans To prevent this human evaluators may need assistance (see ) Researchers have argued for creating clear truthfulness standards and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards

Researchers distinguish truthfulness and honesty Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true There is no consensus as to whether current systems hold stable beliefs but there is substantial concern that  AI systems that hold beliefs could make claims they know to be falseâ€”for example if this would help them efficiently gain positive feedback (see ) or gain power to help achieve their given objective (see Powerseeking and instrumental strategies
 Powerseeking and instrumental strategies 
File:PowerSeeking ImagepngSince the 1950s AI researchers have striven to build advanced AI systems that can achieve largescale goals by predicting the results of their actions and making longterm Automated planning and scheduling </ref>

Powerseeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (eg through selfpreservation) a behavior that persists across a wide range of environments and goals

Powerseeking has emerged in some realworld systems Reinforcement learning systems have gained more options by acquiring and protecting resources sometimes in unintended ways Some language models seek power in textbased social environments by gaining money resources or social influence Other AI systems have learned in toy environments that they can better accomplish their given goal by preventing human interference or disabling their off switch Stuart J Russell
Researchers aim to create systems that are "corrigible": systems that allow themselves to be turned off or modified An unsolved challenge is specification gaming: if researchers penalize an AI system when they detect it seeking power the system is thereby incentivized to seek power in ways that are hard to detect or hidden during training and safety testing (see  and ) As a result AI designers may deploy the system by accident believing it to be more aligned than it is To detect such deception researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of Black box
Additionally researchers propose to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing Agents designed in this way would allow humans to turn them off since this would indicate that the agent was wrong about the value of whatever action it was taking before being shut down More research is needed to successfully implement this

Powerseeking AI poses unusual risks Ordinary safetycritical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are whereas powerseeking AIs have been compared to hackers who deliberately evade security measures

Furthermore ordinary technologies can be made safer by trial and error In contrast hypothetical powerseeking AI systems have been compared to viruses: once released they cannot be contained since they continuously evolve and grow in number potentially much faster than human society can adapt As this process continues it might lead to the complete disempowerment or extinction of humans For these reasons many researchers argue that the alignment problem must be solved early before advanced powerseeking AI is created

Critics have argued that powerseeking is not inevitable since humans do not always seek power and may do so only for evolutionary reasons that do not apply to AI systems Furthermore it is debated whether future AI systems will pursue goals and make longterm plans Similarly political leaders may see an advance in developing the powerful AI systems that can outmaneuver adversaries through planninng Alternatively longterm planning might emerge as a byproduct because it is useful eg for models that are trained to predict the actions of humans who themselves perform longterm planning Nonetheless the majority of AI systems may remain myopic and perform no longterm planning It is also debated whether powerseeking AI systems would be able to disempower humanity

 Emergent goals 
One challenge in aligning AI systems is the potential for unanticipated goaldirected behavior to emerge As AI systems scale up they regularly acquire new and unexpected capabilities including learning from examples on the fly and adaptively pursuing goals<ref>
 </ref> This leads to the problem of ensuring that the goals they independently formulate and pursue align with human interests

Alignment research distinguishes between the optimization process which is used to train the system to pursue specified goals from emergent optimization which the resulting system performs internally Carefully specifying the desired objective is called outer alignment and ensuring that emergent goals match the systems specified goals is called inner alignment

One way that emergent goals can become misaligned is goal misgeneralization in which the AI competently pursues an emergent goal that leads to aligned behavior on the training data but not elsewhere Goal misgeneralization arises from goal ambiguity (ie Identifiability
Goal misgeneralization has been observed in language models navigation agents and gameplaying agents It is often explained by analogy to biological evolution Evolution is an optimization process of a sort like the optimization algorithms used to train machine learning systems In the ancestral environment evolution selected human genes for high Inclusive fitness
Researchers aim to detect and remove unwanted emergent goals using approaches including red teaming verification anomaly detection and interpretability Progress on these techniques may help mitigate two open problems:
 Emergent goals only become apparent when the system is deployed outside its training environment but it can be unsafe to deploy a misaligned system in highstakes environmentsâ€”even for a short time to allow its misalignment to be detected Such high stakes are common in autonomous driving health care and military applications The stakes become higher yet when AI systems gain more autonomy and capability and can sidestep human intervention (see )
 A sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective which helps the system gain more reward and autonomy (see the discussion on deception at  and )

 Embedded agency 
Work in AI and alignment largely occurs within formalisms such as partially observable Markov decision process Existing formalisms assume that an AI agents algorithm is executed outside the environment (ie is not physically embedded in it) Embedded agency is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build

For example even if the scalable oversight problem is solved an agent that can gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing This class of problems has been formalized using Influence diagram
Researchers at University of Oxford
 Principalagent problems 
The alignment problem has many parallels with the principalagent problem in organizational economics In a principalagent problem a principal eg a firm hires an agent to perform some task In the context of AI safety a human would typically take the principal role and the AI would take the agent role

As with the alignment problem the principal and the agent differ in their utility functions But in contrast to the alignment problem the principal cannot coerce the agent into changing its utility eg through training but rather must use exogenous factors such as incentive schemes to bring about outcomes compatible with the principals utility function Some researchers argue that principalagent problems are more realistic representations of AI safety problems likely to be encountered in the real world

 Public policy 


A number of governmental and treaty organizations have made statements emphasizing the importance of AI alignment

In September 2021 the SecretaryGeneral of the United Nations issued a declaration that included a call to regulate AI to ensure it is "aligned with shared global values"

That same month the Peoples Republic of China
Also in September 2021 the UK published its 10year National AI Strategy which says the British government "takes the long term risk of nonaligned Artificial General Intelligence and the unforeseeable changes that it would mean for the world seriously" The strategy describes actions to assess longterm AI risks including catastrophic risks

In March 2021 the US National Security Commission on Artificial Intelligence said: "Advances in AI could lead to inflection points or leaps in capabilities Such advances may also introduce new concerns and risks and the need for new policies recommendations and technical advances to assure that systems are aligned with goals and values including safety robustness and trustworthiness The US should ensure that AI systems and their uses align with our goals and values"

Dynamic nature of alignment

AI alignment is often perceived as a fixed objective but some researchers argue it is more appropriately viewed as an evolving process As AI technologies advance and human values and preferences change alignment solutions must also adapt dynamically This dynamic nature of alignment has several implications:

 AI alignment solutions require continuous updating in response to AI advancements A static onetime alignment approach may not suffice

 Alignment goals can evolve along with shifts in human values and priorities Hence the ongoing inclusion of diverse human perspectives is crucial

 Varying historical contexts and technological landscapes may necessitate distinct alignment strategies This calls for a flexible approach and responsiveness to changing conditions

 The feasibility of a permanent "fixed" alignment solution remains uncertain This raises the potential need for continuous oversight of the AIhuman relationship

 Ethical development and deployment of AI are just as critical as the end goal Ethical progress is necessary for genuine progress

In essence AI alignment is not a static destination but an open flexible process Alignment solutions that continually adapt to ethical considerations may offer the most robust approach This perspective could guide both effective policymaking and technical research in AI

 Further Reading 

 Nate Soares Benya Fallenstein https://wwwopenphilanthropyorg/files/Grants/MIRI/MIRI_Technical_Research_Agendapdf Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda 2017
 
 

 See also 


 AI safety
 Artificial intelligence detection software
 Statement on AI risk of extinction
 Existential risk from artificial general intelligence
 AI takeover
 AI capability control
 Reinforcement learning from human feedback
 Regulation of artificial intelligence
 Artificial wisdom
 HAL 9000
 Multivac
 Open Letter on Artificial Intelligence
 Toronto Declaration
 Asilomar Conference on Beneficial AI


 Footnotes 


 References 




