
Machine ethics (or machine morality computational morality or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of design
History
Before the 21st century the ethics of machines had largely been the subject of science fiction literature mainly due to computing and artificial intelligence (AI) limitations Although the definition of "Machine Ethics" has evolved since the term was coined by Mitchell Waldrop in the 1987 AI Magazine article "A Question of Responsibility":<blockquote>"However one thing that is apparent from the above discussion is that intelligent machines will embody values assumptions and purposes whether their programmers consciously intend them to or not Thus as computers and robots become more and more intelligent it becomes imperative that we think carefully and explicitly about what those builtin values are Perhaps what we need is in fact a theory and practice of machine ethics in the spirit of Asimov’s Three Laws of Robotics
In 2004 Towards Machine Ethics  was presented at the AAAI Workshop on Agent Organizations: Theory and Practice  in which theoretical foundations for machine ethics were laid out

It was in the AAAI Fall 2005 Symposium on Machine Ethics where researchers met for the first time to consider implementation of an ethical dimension in autonomous systems  A variety of perspectives of this nascent field can be found in the collected edition "Machine Ethics" that stems from the AAAI Fall 2005 Symposium on Machine Ethics

In 2007 AI Magazine featured Machine Ethics: Creating an Ethical Intelligent Agent an article that
discussed the importance of machine ethics the need for machines that represent ethical principles explicitly and the challenges facing those working on machine ethics It also demonstrated that it is possible at least in a limited domain for a machine to abstract an ethical principle from examples of ethical judgments and use that principle to guide its own behavior

In 2009 Oxford University Press published Moral Machines Teaching Robots Right from Wrong which it advertised as "the first book to examine the challenge of building artificial moral agents probing deeply into the nature of human decision making and ethics" It cited some 450 sources about 100 of which addressed Focuses of machine ethics
In 2011 Cambridge University Press published a collection of essays about machine ethics edited by Michael and Susan Leigh Anderson The collection consists of the challenges of adding ethical principles to machines

In 2014 the US Office of Naval Research announced that it would distribute $75 million in grants over five years to university researchers to study questions of machine ethics as applied to autonomous robots and Nick Bostroms Superintelligence: Paths Dangers Strategies which raised machine ethics as the "most importantissue humanity has ever faced" reached 17 on the New York Times list of best selling science books

In 2016 the European Parliament published a paper (22page PDF) to encourage the Commission to address the issue of robots legal status as described more briefly in the press This paper included sections regarding the legal liabilities of robots in which the liabilities were argued as being proportional to the robots level of autonomy The paper also brought into question the number of jobs that could be replaced by AI robots

In 2019 the Proceedings of the IEEE published a special issue on Machine Ethics: The Design and Governance of Ethical AI and Autonomous Systems edited by Alan Winfield Katina Michael Jeremy Pitt and Vanessa Evers "The issue includes papers describing implicit ethical agents where machines are designed to avoid unethical outcomes as well as explicit ethical agents or machines that either encode or learn ethics and determine actions based on those ethics"

Definitions
James H Moor one of the pioneering theoreticians in the field of computer ethics defines four kinds of ethical robots As an extensive researcher on the studies of philosophy of artificial intelligence philosophy of mind philosophy of science and logic Moor defines machines to be ethical impact agents implicit ethical agents explicit ethical agents or full ethical agents A machine can be more than one type of agent
Ethical impact agents: These are machine systems that carry an ethical impact whether intended or not At the same time these agents have the potential to act unethical Moor gives a hypothetical example called the Goodman agent named after philosopher Nelson Goodman The Goodman agent compares dates but has the Year 2000 problemImplicit ethical agents: For the consideration of human safety these agents are programmed to have a failsafe or a builtin virtue They are not entirely ethical in nature but rather programmed to avoid unethical outcomes
Explicit ethical agents: These are machines that are capable of processing scenarios and acting on ethical decisions Machines which have algorithms to act ethically
Full ethical agents: These machines are similar to explicit ethical agents in being able to make ethical decisions However they also contain human Metaphysics(See Moral responsibilityArtificial systems
Focuses of machine ethics

 AI control problem 


Some scholars such as philosopher Nick Bostrom and AI researcher Stuart J Russell
This presents the AI control problem: how to build an intelligent agent that will aid its creators while avoiding inadvertently building a superintelligence that will harm its creators The danger of not designing control right "the first time" is that a superintelligence may be able to seize power over its environment and prevent humans from shutting it down Potential AI control strategies include "capability control" (limiting an AIs ability to influence the world) and "motivational control" (one way of building an AI whose goals are AI alignment
Algorithms and training

AI paradigms have been debated over especially in relation to their efficacy and bias Nick Bostrom and Eliezer Yudkowsky have argued for decision trees (such as ID3 algorithm
In 2009 in an experiment at the Laboratory of Intelligent Systems in the École Polytechnique Fédérale de Lausanne
 Autonomous weapons systems 



In 2009 academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become selfsufficient and able to make their own decisions They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy and to what degree they could use such abilities to possibly pose any threat or hazard They noted that some machines have acquired various forms of semiautonomy including being able to find power sources on their own and being able to independently choose targets to attack with weapons They also noted that some computer viruses can evade elimination and have achieved "cockroach intelligence" They noted that selfawareness as depicted in sciencefiction is probably unlikely but that there were other potential hazards and pitfalls

Some experts and academics have questioned the use of robots for military combat especially when such robots are given some degree of autonomous functions The US Navy has funded a report which indicates that as military robots become more complex there should be greater attention to implications of their ability to make autonomous decisions The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue They point to programs like the Language Acquisition Device which can emulate human interaction

 Integration of Artificial General Intelligences with society 
Preliminary work has been conducted on methods of integrating artificial general intelligences (full ethical agents as defined above) with existing legal and social frameworks Approaches have focused on consideration of their legal position and rights

 Machine learning bias 

Big data and machine learning algorithms have become popular among numerous industries including online advertising credit ratings and criminal sentencing with the promise of providing more objective datadriven results but have been identified as a potential source for perpetuating social inequalities and discrimination  A 2015 study found that women were less likely to be shown highincome job ads by Googles AdSense Another study found that Amazoncom
The United States judicial system has begun using quantitative risk assessment software when making decisions related to releasing people on bail and sentencing in an effort to be more fair and to reduce an already high United States incarceration rate
In 2016 the Obama Administrations Big Data Working Group—an overseer of various bigdata regulatory frameworks—released reports arguing “the potential of encoding discrimination in automated decisions” and calling for “equal opportunity by design” for applications such as credit scoring The reports encourage discourse among Policymakers
Ethical frameworks and practices

Practices
In March 2018 in an effort to address rising concerns over machine learnings impact on human rights the World Economic Forum and Global Future Council on Human Rights published a white paper with detailed recommendations on how best to prevent discriminatory outcomes in machine learning The World Economic Forum developed four recommendations based on the UN Guiding Principles of Human Rights to help address and prevent discriminatory outcomes in machine learning

The World Economic Forums recommendations are as follows:

 Active Inclusion: the development and design of machine learning applications must actively seek a diversity of input especially of the norms and values of specific populations affected by the output of AI systems
 Fairness (machine learning) Right to Understanding: Involvement of machine learning systems in decisionmaking that affects individual rights must be disclosed and the systems must be able to provide an explanation of their decisionmaking that is understandable to end users and reviewable by a competent human authority Where this is impossible and rights are at stake leaders in the design deployment and regulation of machine learning technology must question whether or not it should be used
 Access to Redress: Leaders designers and developers of machine learning systems are responsible for identifying the potential negative human rights impacts of their systems They must make visible avenues for redress for those affected by disparate impacts and establish processes for the timely redress of any discriminatory outputs

In January 2020 Harvard Universitys Berkman Klein Center for Internet & Society
Approaches
There have been several attempts to make ethics computable or at least formal ethics
One thought experiment focuses on a Genie Golem with unlimited powers presenting itself to the reader This Genie declares that it will return in 50 years and demands that it be provided with a definite set of morals that it will then immediately act upon The purpose of this experiment is to initiate a discourse over how best to handle defining complete set of ethics that computers may understand

In fiction
In science fiction movies and novels have played with the idea of sentience in robots and machines

Neill Blomkamp
Isaac Asimov considered the issue in the 1950s in I Robot At the insistence of his editor John W Campbell Jr he proposed the Three Laws of Robotics to govern artificially intelligent systems Much of his work was then spent testing the boundaries of his three laws to see where they would break down or where they would create paradoxical or unanticipated behavior His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances<ref nameAsimov2008>Cite book
       
</ref> In Philip K Dick
Related fields

 Affective computing
 Formal ethics
 Bioethics
 Computational theory of mind
 Computer ethics
 Ethics of artificial intelligence
 Moral psychology
 Philosophy of artificial intelligence
 Philosophy of mind

See also

 Artificial intelligence
 AI safety
 AI takeover
 Artificial intelligence in fiction
 Friendly artificial intelligence
 David M EddyArchimedes2C Inc Google car
 Military robot
 Machine Intelligence Research Institute
 Robot ethics
 Space law
 Selfreplicating spacecraft
 Watson (computer) Tay (bot)

Notes


External links
 http://wwwmachineethicscom/ Machine Ethics Interdisciplinary project on machine ethics
 https://machineethicsnet/ The Machine Ethics Podcast Podcast discussing Machine Ethics AI and Tech ethics

References
 Wallach Wendell; Allen Colin (November 2008) Moral Machines: Teaching Robots Right from Wrong USA: Oxford University Press
 Anderson Michael; Anderson Susan Leigh eds (July 2011) Machine Ethics Cambridge University Press 
 Storrs Hall J (May 30 2007) Beyond AI: Creating the Conscience of the Machine Prometheus Books
 Moor J (2006) The Nature Importance and Difficulty of Machine Ethics IEEE Intelligent Systems 21(4) pp&nbsp;18–21
 Anderson M and Anderson S (2007) Creating an Ethical Intelligent Agent AI Magazine Volume 28(4)

Further reading
 Hagendorff Thilo (2021) Linking Human And Machine Behavior: A New Approach to Evaluate Training Data Quality for Beneficial Machine Learning Minds and Machines  
 Anderson Michael; Anderson Susan Leigh eds (July/August 2006) "https://webarchiveorg/web/20111126025029/http://wwwcomputerorg/portal/web/csdl/abs/mags/ex/2006/04/x4tochtm Special Issue on Machine Ethics" IEEE Intelligent Systems 21 (4): 10–63
 Bendel Oliver (December 11 2013) Considerations about the Relationship between Animal and Machine Ethics AI & SOCIETY 
 Dabringer Gerhard ed (2010) "https://webarchiveorg/web/20120617002131/http://isrnu/robots/Ethica_Ethics_UnmannedSystemspdf Ethical and Legal Aspects of Unmanned Systems Interviews" Austrian Ministry of Defence and Sports Vienna 2010 
 Gardner A (1987) An Artificial Approach to Legal Reasoning Cambridge MA: MIT Press
 Georges T M (2003) Digital Soul: Intelligent Machines and Human Values Cambridge MA: Westview Press
 Singer PW (December 29 2009) Wired for War: The Robotics Revolution and Conflict in the 21st Century: Penguin Group Winfield A Michael K Pitt J and Evers V (March 2019) Special Issue on Machine Ethics: The Design and Governance of Ethical AI and Autonomous Systems Proceedings of the IEEE 107 (3): 501–615 




