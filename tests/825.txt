


Friendly artificial intelligence (also friendly AI or FAI) is hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least AI alignment
 Etymology and usage 
File:Eliezer Yudkowsky Stanford 2006 (square crop)jpgThe term was coined by Eliezer Yudkowsky who is best known for popularizing the idea to discuss superintelligence
<blockquote>Yudkowsky (2008) goes into more detail about how to design a Friendly AI He asserts that friendliness (a desire not to harm humans) should be designed in from the start but that the designers should recognize both that their own designs may be flawed and that the robot will learn and evolve over time Thus the challenge is one of mechanism design&mdash;to define a mechanism for evolving AI systems under a system of checks and balances and to give the systems utility functions that will remain friendly in the face of such changes</blockquote>

Friendly is used in this context as technical terminology and picks out agents that are safe and useful not necessarily ones that are "friendly" in the colloquial sense The concept is primarily invoked in the context of discussions of recursively selfimproving artificial agents that rapidly intelligence explosion
 Risks of unfriendly AI 

The roots of concern about artificial intelligence are very old Kevin LaGrandeur showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the golem or the protorobots of Gerbert of Aurillac and Roger Bacon  In those stories the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as subhuman) and cause disastrous conflict By 1942 these themes prompted Isaac Asimov to create the "Three Laws of Robotics"—principles hardwired into all the robots in his fiction intended to prevent them from turning on their creators or allowing them to come to harm
 
In modern times as the prospect of Superintelligence
<blockquote>Basically we should assume that a superintelligence would be able to achieve whatever goals it has Therefore it is extremely important that the goals we endow it with and its entire motivation system is human friendly</blockquote>

In 2008 Eliezer Yudkowsky called for the creation of "friendly AI" to mitigate existential risk from advanced artificial intelligence He explains: "The AI does not hate you nor does it love you but you are made out of atoms which it can use for something else"

Steve Omohundro says that a sufficiently advanced AI system will unless explicitly counteracted exhibit a number of Instrumental convergenceBasic AI drives
Alexander WissnerGross says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold and unfriendly if their planning horizon is shorter than that threshold

Luke Muehlhauser writing for the Machine Intelligence Research Institute recommends that machine ethics researchers adopt what Bruce Schneier has called the "security mindset": Rather than thinking about how a system will work imagine how it could fail For instance he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm

In 2014 Luke Muehlhauser and Nick Bostrom underlined the need for friendly AI; nonetheless the difficulties in designing a friendly superintelligence for instance via programming counterfactual moral thinking are considerable

Coherent extrapolated volition
Yudkowsky advances the Coherent Extrapolated Volition (CEV) model According to him our coherent extrapolated volition is "our wish if we knew more thought faster were more the people we wished we were had grown up farther together; where the extrapolation converges rather than diverges where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated interpreted as we wish that interpreted"

Rather than a Friendly AI being designed directly by human programmers it is to be designed by a "seed AI" programmed to first study human nature and then produce the AI which humanity would want given sufficient time and insight to arrive at a satisfactory answer The appeal to an evolutionary psychology
Other approaches

Steve Omohundro has proposed a "scaffolding" approach to AI safety in which one provably safe AI generation helps build the next provably safe generation

Seth Baum argues that the development of safe socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of AI research communities and so can be constrained by extrinsic measures and motivated by intrinsic measures Intrinsic motivations can be strengthened when messages resonate with AI developers; Baum argues that in contrast "existing messages about beneficial AI are not always framed well" Baum advocates for "cooperative relationships and positive framing of AI researchers" and cautions against characterizing AI researchers as "not want(ing) to pursue beneficial designs"

In his book Human Compatible AI researcher Stuart J Russell lists three principles to guide the development of beneficial machines  He emphasizes that these principles are not meant to be explicitly coded into the machines; rather they are intended for the human developers  The principles are as follows:

quote
2 The machine is initially uncertain about what those preferences are

3 The ultimate source of information about human preferences is human behavior

The "preferences" Russell refers to "are allencompassing; they cover everything you might care about arbitrarily far into the future"  Similarly "behavior" includes any choice between options and the uncertainty is such that some probability which may be quite small must be assigned to every logically possible human preference

Public policy
James Barrat author of Our Final Invention suggested that "a publicprivate partnership has to be created to bring AImakers together to share ideas about security—something like the International Atomic Energy Agency but in partnership with corporations" He urges AI researchers to convene a meeting similar to the Asilomar Conference on Recombinant DNA which discussed risks of biotechnology 

John McGinnis encourages governments to accelerate friendly AI research Because the goalposts of friendly AI are not necessarily eminent he suggests a model similar to the National Institutes of Health where "Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards" McGinnis feels that peer review is better "than regulation to address technical issues that are not possible to capture through bureaucratic mandates" McGinnis notes that his proposal stands in contrast to that of the Machine Intelligence Research Institute which generally aims to avoid government involvement in friendly AI

Criticism


Some critics believe that both humanlevel AI and superintelligence are unlikely and that therefore friendly AI is unlikely Writing in The Guardian Alan Winfield compares humanlevel artificial intelligence with fasterthanlight travel in terms of difficulty and states that while we need to be "cautious and prepared" given the stakes involved we "dont need to be obsessing" about the risks of superintelligence Boyles and Joaquin on the other hand argue that Luke Muehlhauser and Nick Bostrom’s proposal to create friendly AIs appear to be bleak This is because Muehlhauser and Bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had In an article in AI & Society Boyles and Joaquin maintain that such AIs would not be that friendly considering the following: the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine the difficulty of cashing out the set of moral values—that is those that are more ideal than the ones human beings possess at present and the apparent disconnect between counterfactual antecedents and ideal value consequent

Some philosophers claim that any truly "rational" agent whether artificial or human will naturally be benevolent; in this view deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful Other critics question whether it is possible for an artificial intelligence to be friendly Adam Keiper and Ari N Schulman editors of the technology journal The New Atlantis (journal)
See also

 Affective computing
 AI alignment
 AI effect
 AI takeover
 Ambient intelligence
 Applications of artificial intelligence
 Artificial intelligence arms race
 Artificial intelligence systems integration
 Autonomous agent
 Embodied agent
 Emotion recognition
 Existential risk from artificial general intelligence
 Hallucination (artificial intelligence)
 Hybrid intelligent system
 Intelligence explosion
 Intelligent agent
 Intelligent control
 Machine ethics
 Machine Intelligence Research Institute
 OpenAI
 Regulation of algorithms
 Rokos basilisk
 Sentiment analysis
 Singularitarianism – a moral philosophy advocated by proponents of Friendly AI
 Suffering risks
 Technological singularity
 Three Laws of Robotics


References


Further reading
 Yudkowsky E http://intelligenceorg/files/AIPosNegFactorpdf Artificial Intelligence as a Positive and Negative Factor in Global Risk In Global Catastrophic Risks Oxford University Press 2008<br />Discusses Artificial Intelligence from the perspective of Existential risk  In particular Sections 14 give background to the definition of Friendly AI in Section 5  Section 6 gives two classes of mistakes (technical and philosophical) which would both lead to the accidental creation of nonFriendly AIs  Sections 713 discuss further related issues
 Omohundro S  2008 The Basic AI Drives Appeared in AGI08  Proceedings of the First Conference on Artificial General Intelligence
 Mason C 2008 https://aaaiorg/Papers/Workshops/2008/WS0807/WS0807023pdf HumanLevel AI Requires Compassionate Intelligence  Appears in AAAI 2008 Workshop on MetaReasoning:Thinking About Thinking
 Froding B and Peterson M 2021 https://linkspringercom/article/101007/s1067602009556w Friendly AI Ethics and Information Technology volume 23 pp 207–214

External links
 https://nickbostromcom/ethics/ai Ethical Issues in Advanced Artificial Intelligence by Nick Bostrom
 https://intelligenceorg/iefaq/WhatIsFriendlyAI What is Friendly AI? &mdash; A brief description of Friendly AI by the Machine Intelligence Research Institute
 https://intelligenceorg/files/CFAIpdf Creating Friendly AI 10: The Analysis and Design of Benevolent Goal Architectures &mdash; A near booklength description from the MIRI
 http://wwwssecwiscedu/~billh/g/SIAI_critiquehtml Critique of the MIRI Guidelines on Friendly AI &mdash; by Bill Hibbard
 http://wwwoptimalorg/peter/siai_guidelineshtm Commentary on MIRIs Guidelines on Friendly AI &mdash; by Peter Voss
 https://wwwthenewatlantiscom/publications/theproblemwithfriendlyartificialintelligence The Problem with ‘Friendly’ Artificial Intelligence &mdash; On the motives for and impossibility of FAI; by Adam Keiper and Ari N Schulman




